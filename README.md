# ğŸ” Explainability in Vision Transformers

This repository contains an interpretability pipeline for Vision Transformers (ViTs) using a pretrained **DeiT-Tiny** model. It implements both **Attention Rollout** and **Gradient Attention Rollout**, and introduces a quantitative metric â€” **Foregroundâ€“Background Attention Fraction (FAF/BAF)** â€” using **Mask R-CNN** to measure how well ViTs focus on meaningful object regions.

The project also compares explanation behaviour between **CNNs ğŸ§ ** and **Vision Transformers ğŸ¤–**, highlighting how transformers rely on global patch interactions rather than localized feature extraction. Experiments show that **gradient-based rollout achieves ~25% higher foreground alignment**, producing more precise and class-specific attribution maps.

---

## âœ¨ Features
- ğŸ”„ Attention Rollout & Gradient Attention Rollout implementations  
- ğŸ§© Patch-level attention visualization on the 14Ã—14 ViT grid  
- ğŸ¯ FAF/BAF metric using Mask R-CNN for quantitative semantic focus evaluation  
- âš–ï¸ Comparison of interpretability behaviour between CNNs and ViTs  
- ğŸ“ˆ Reproducible experiments with visualizations  

---



